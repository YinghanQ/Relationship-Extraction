{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b75518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\envs\\test\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\envs\\test\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\envs\\test\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\envs\\test\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\test\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\test\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in d:\\anaconda\\envs\\test\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: datasets in d:\\anaconda\\envs\\test\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (0.29.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\envs\\test\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\test\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\test\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\test\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda\\envs\\test\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\test\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda\\envs\\test\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\anaconda\\envs\\test\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install datasets \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44c5088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382509d1",
   "metadata": {},
   "source": [
    "# 1. Original Dataset\n",
    "Loading Original Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142e72ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'The <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.', 'The <e1>author</e1> of a keygen uses a <e2>disassembler</e2> to look at the raw assembly code.', 'A misty <e1>ridge</e1> uprises from the <e2>surge</e2>.', 'The <e1>student</e1> <e2>association</e2> is the voice of the undergraduate student population of the State University of New York at Buffalo.'], 'relation': [3, 18, 11, 18, 12]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "# Display the first few rows of the training set\n",
    "print(ds['train'][:5])  # This will display the first 5 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf44fe3",
   "metadata": {},
   "source": [
    "# 2. Data Preprocess\n",
    "\n",
    "The purpose of this code is to load and process the SemEval-2010 Task 8 dataset, preparing it for training, validation, and testing of a relationship classification model. \n",
    "\n",
    "1. **Loading Data:**\n",
    "   - If local training, validation, and test data files (`train.json`, `validation.json`, `test.json`) already exist, the code will directly load these files.\n",
    "   - If the local data files do not exist, the code will download the training and test data from the publicly available dataset on Hugging Face, `SemEvalWorkshop/sem_eval_2010_task_8`.\n",
    "\n",
    "2. **Data Cleaning:**\n",
    "   - **Cleaning Text**: The text in the data is cleaned (e.g., removing HTML escape characters, extra spaces, etc.) to ensure uniform text formatting.\n",
    "   - **Extracting Entities**: The entities between the `<e1>` and `<e2>` tags in the sentences are extracted using regular expressions, and the sentences are tokenized.\n",
    "\n",
    "3. **Data Preprocessing:**\n",
    "   - **Label Conversion**: Relation labels are converted from numeric IDs to human-readable relation types (e.g., “Cause-Effect(e1,e2)”).\n",
    "   - **Handling Rare Relations**: Relations that appear only once in the test or validation sets are handled separately to prevent data imbalance.\n",
    "   - **Data Splitting**: **The original test data is further split into a validation set and a test set with a radio: 60% - 40%**, ensuring balanced class distribution (using `train_test_split`).\n",
    "\n",
    "4. **Saving Data:**\n",
    "   - The cleaned and processed data is saved in JSON format for easy use in training models later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8bbea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train and test data found locally. Loading...\n",
      "Processed data saved!\n",
      "Training set size: 8000\n",
      "Validation set size: 1629\n",
      "Test set size: 1088\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define 19 relationship labels\n",
    "relations = [\n",
    "    \"Cause-Effect(e1,e2)\", \"Cause-Effect(e2,e1)\",\n",
    "    \"Component-Whole(e1,e2)\", \"Component-Whole(e2,e1)\",\n",
    "    \"Content-Container(e1,e2)\", \"Content-Container(e2,e1)\",\n",
    "    \"Entity-Destination(e1,e2)\", \"Entity-Destination(e2,e1)\",\n",
    "    \"Entity-Origin(e1,e2)\", \"Entity-Origin(e2,e1)\",\n",
    "    \"Instrument-Agency(e1,e2)\", \"Instrument-Agency(e2,e1)\",\n",
    "    \"Member-Collection(e1,e2)\", \"Member-Collection(e2,e1)\",\n",
    "    \"Message-Topic(e1,e2)\", \"Message-Topic(e2,e1)\",\n",
    "    \"Product-Producer(e1,e2)\", \"Product-Producer(e2,e1)\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "# Relationship label mapping\n",
    "label2id = {label: i for i, label in enumerate(relations)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# Define file paths\n",
    "original_train_file = \"original_train.json\"\n",
    "original_test_file = \"original_test.json\"\n",
    "train_file = \"train.json\"\n",
    "valid_file = \"validation.json\"\n",
    "test_file = \"test.json\"\n",
    "\n",
    "def load_json_lines(file_path):\n",
    "    \"\"\"Load local JSON Lines data\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))  \n",
    "    return data\n",
    "    \n",
    "def load_local_data(file_path):\n",
    "    \"\"\"Load JSON data locally (single JSON object)\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "    \n",
    "def save_local_data(data, file_path):\n",
    "    \"\"\"Save data in JSON Lines format\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")   \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text data\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return None  # Filter out empty text\n",
    "    text = html.unescape(text)  # Handle HTML escape characters\n",
    "    return text.strip()\n",
    "\n",
    "def search_entity(sentence):\n",
    "    \"\"\"Extract and format entity markers in the sentence\"\"\"\n",
    "    e1 = re.findall(r'<e1>(.*)</e1>', sentence)[0]\n",
    "    e2 = re.findall(r'<e2>(.*)</e2>', sentence)[0]\n",
    "    sentence = sentence.replace('<e1>' + e1 + '</e1>', ' <e1> ' + e1 + ' </e1> ', 1)\n",
    "    sentence = sentence.replace('<e2>' + e2 + '</e2>', ' <e2> ' + e2 + ' </e2> ', 1)\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    sentence = sentence.replace('< e1 >', '<e1>')\n",
    "    sentence = sentence.replace('< e2 >', '<e2>')\n",
    "    sentence = sentence.replace('< /e1 >', '</e1>')\n",
    "    sentence = sentence.replace('< /e2 >', '</e2>')\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    assert '<e1>' in sentence\n",
    "    assert '<e2>' in sentence\n",
    "    assert '</e1>' in sentence\n",
    "    assert '</e2>' in sentence\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "if os.path.exists(original_train_file) and os.path.exists(original_test_file):\n",
    "    print(\"Original train and test data found locally. Loading...\")\n",
    "    train_data_with_ids = load_json_lines(original_train_file)\n",
    "    test_data_with_ids = load_json_lines(original_test_file)\n",
    "else:\n",
    "    print(\"Loading dataset from Hugging Face...\")\n",
    "    ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "    train_texts_all = ds[\"train\"][\"sentence\"]\n",
    "    train_labels_all = ds[\"train\"][\"relation\"]\n",
    "    test_texts = ds[\"test\"][\"sentence\"]\n",
    "    test_labels = ds[\"test\"][\"relation\"]\n",
    "\n",
    "    # **Create original data with IDs**\n",
    "    train_data_with_ids = [\n",
    "        {\"id\": i + 1, \"sentence\": text, \"relation\": label} \n",
    "        for i, (text, label) in enumerate(zip(train_texts_all, train_labels_all))\n",
    "    ]\n",
    "    \n",
    "    test_data_with_ids = [\n",
    "        {\"id\": i + 8001, \"sentence\": text, \"relation\": label} \n",
    "        for i, (text, label) in enumerate(zip(test_texts, test_labels))\n",
    "    ]\n",
    "\n",
    "    # **Save original data**\n",
    "    save_local_data(train_data_with_ids, original_train_file)\n",
    "    save_local_data(test_data_with_ids, original_test_file)\n",
    "    print(\"Original train and test data saved locally!\")\n",
    "\n",
    "# **Step 2: Clean the data**\n",
    "clean_train_data = [(clean_text(d[\"sentence\"]), d[\"relation\"], d[\"id\"]) for d in train_data_with_ids]\n",
    "clean_train_data = [x for x in clean_train_data if x[0] is not None]\n",
    "    \n",
    "clean_test_data = [(clean_text(d[\"sentence\"]), d[\"relation\"], d[\"id\"]) for d in test_data_with_ids]\n",
    "clean_test_data = [x for x in clean_test_data if x[0] is not None]\n",
    "        \n",
    "# **Step 3: New data split**\n",
    "# 1. Read original test set data\n",
    "test_data = [{\"id\": item[2], \"relation\": item[1], \"sentence\": search_entity(item[0]), \"comment\": \"N/A\"} for item in clean_test_data]\n",
    "    \n",
    "# 2. Get category labels (relation)\n",
    "labels = [item[\"relation\"] for item in test_data]\n",
    "    \n",
    "# 3. Count occurrences of each category\n",
    "relation_counts = Counter(labels)\n",
    "    \n",
    "# 4. Select categories that appear only once\n",
    "single_instance_relations = {relation for relation, count in relation_counts.items() if count == 1}\n",
    "    \n",
    "# 5. Separate data\n",
    "single_instance_data = [item for item in test_data if item[\"relation\"] in single_instance_relations]  # Directly into new_test\n",
    "remaining_data = [item for item in test_data if item[\"relation\"] not in single_instance_relations]  # Used in train_test_split\n",
    "    \n",
    "# 6. Perform 60%-40% stratified sampling\n",
    "if remaining_data:\n",
    "    remaining_labels = [item[\"relation\"] for item in remaining_data]\n",
    "    new_val_data, new_test_data_split = train_test_split(\n",
    "            remaining_data, test_size=0.4, stratify=remaining_labels, random_state=42\n",
    "            )\n",
    "else:\n",
    "    new_val_data, new_test_data_split = [], []\n",
    "\n",
    "# 7. Merge final test set data\n",
    "new_test_data = new_test_data_split + single_instance_data\n",
    "    \n",
    "# **Step 4: Convert to final format**\n",
    "train_data = [{\n",
    "            \"id\": item[2],\n",
    "            \"relation\": id2label[item[1]],\n",
    "            \"sentence\": search_entity(item[0]),\n",
    "            # \"comment\": \"N/A\"\n",
    "        } for item in clean_train_data]\n",
    "    \n",
    "valid_data = [{\n",
    "            \"id\": item[\"id\"],  \n",
    "            \"relation\": id2label.get(item[\"relation\"]),\n",
    "            \"sentence\": item[\"sentence\"],  \n",
    "            # \"comment\": \"N/A\"\n",
    "        } for item in new_val_data]\n",
    "    \n",
    "test_data = [{\n",
    "            \"id\": item[\"id\"],  \n",
    "            \"relation\": id2label.get(item[\"relation\"]),\n",
    "            \"sentence\": item[\"sentence\"],  \n",
    "            # \"comment\": \"N/A\"\n",
    "        } for item in new_test_data]\n",
    "    \n",
    "# **Step 5: Save the cleaned data**\n",
    "save_local_data(train_data, train_file)\n",
    "save_local_data(valid_data, valid_file)\n",
    "save_local_data(test_data, test_file)\n",
    "    \n",
    "print(\"Processed data saved!\")\n",
    "print(\"Training set size:\", len(train_data))\n",
    "print(\"Validation set size:\", len(new_val_data))\n",
    "print(\"Test set size:\", len(new_test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb723b",
   "metadata": {},
   "source": [
    "# 3. Display Processed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade5a85",
   "metadata": {},
   "source": [
    "## 3.1 Display JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8e8067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'relation': 'Component-Whole(e2,e1)', 'sentence': ['The', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', '<e1>', 'configuration', '</e1>', 'of', 'antenna', '<e2>', 'elements', '</e2>', '.']}, {'id': 2, 'relation': 'Other', 'sentence': ['The', '<e1>', 'child', '</e1>', 'was', 'carefully', 'wrapped', 'and', 'bound', 'into', 'the', '<e2>', 'cradle', '</e2>', 'by', 'means', 'of', 'a', 'cord', '.']}, {'id': 3, 'relation': 'Instrument-Agency(e2,e1)', 'sentence': ['The', '<e1>', 'author', '</e1>', 'of', 'a', 'keygen', 'uses', 'a', '<e2>', 'disassembler', '</e2>', 'to', 'look', 'at', 'the', 'raw', 'assembly', 'code', '.']}, {'id': 4, 'relation': 'Other', 'sentence': ['A', 'misty', '<e1>', 'ridge', '</e1>', 'uprises', 'from', 'the', '<e2>', 'surge', '</e2>', '.']}, {'id': 5, 'relation': 'Member-Collection(e1,e2)', 'sentence': ['The', '<e1>', 'student', '</e1>', '<e2>', 'association', '</e2>', 'is', 'the', 'voice', 'of', 'the', 'undergraduate', 'student', 'population', 'of', 'the', 'State', 'University', 'of', 'New', 'York', 'at', 'Buffalo', '.']}, {'id': 6, 'relation': 'Other', 'sentence': ['This', 'is', 'the', 'sprawling', '<e1>', 'complex', '</e1>', 'that', 'is', 'Peru', \"'s\", 'largest', '<e2>', 'producer', '</e2>', 'of', 'silver', '.']}, {'id': 7, 'relation': 'Cause-Effect(e2,e1)', 'sentence': ['The', 'current', 'view', 'is', 'that', 'the', 'chronic', '<e1>', 'inflammation', '</e1>', 'in', 'the', 'distal', 'part', 'of', 'the', 'stomach', 'caused', 'by', 'Helicobacter', 'pylori', '<e2>', 'infection', '</e2>', 'results', 'in', 'an', 'increased', 'acid', 'production', 'from', 'the', 'non-infected', 'upper', 'corpus', 'region', 'of', 'the', 'stomach', '.']}, {'id': 8, 'relation': 'Entity-Destination(e1,e2)', 'sentence': ['<e1>', 'People', '</e1>', 'have', 'been', 'moving', 'back', 'into', '<e2>', 'downtown', '</e2>', '.']}, {'id': 9, 'relation': 'Content-Container(e1,e2)', 'sentence': ['The', '<e1>', 'lawsonite', '</e1>', 'was', 'contained', 'in', 'a', '<e2>', 'platinum', 'crucible', '</e2>', 'and', 'the', 'counter-weight', 'was', 'a', 'plastic', 'crucible', 'with', 'metal', 'pieces', '.']}, {'id': 10, 'relation': 'Entity-Destination(e1,e2)', 'sentence': ['The', 'solute', 'was', 'placed', 'inside', 'a', 'beaker', 'and', '5', 'mL', 'of', 'the', '<e1>', 'solvent', '</e1>', 'was', 'pipetted', 'into', 'a', '25', 'mL', 'glass', '<e2>', 'flask', '</e2>', 'for', 'each', 'trial', '.']}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json_lines(file_path):\n",
    "    \"\"\"Load JSON data locally (multiple JSON objects)\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))  # Read and parse each line as a JSON object\n",
    "    return data\n",
    "\n",
    "# Example of loading data from a JSON file with multiple objects per line\n",
    "file_path = 'train.json'  # Change this to your file path\n",
    "data = load_json_lines(file_path)\n",
    "\n",
    "# Print the first few records to inspect\n",
    "print(data[:10])  # Display the first ten elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1965059",
   "metadata": {},
   "source": [
    "## 3.2 Distriution of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26dd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Original Datasets distribution-----------\n",
      "Training set relation distribution:\n",
      "Component-Whole(e2,e1): 471\n",
      "Other: 1410\n",
      "Instrument-Agency(e2,e1): 407\n",
      "Member-Collection(e1,e2): 78\n",
      "Cause-Effect(e2,e1): 659\n",
      "Entity-Destination(e1,e2): 844\n",
      "Content-Container(e1,e2): 374\n",
      "Message-Topic(e1,e2): 490\n",
      "Product-Producer(e2,e1): 394\n",
      "Member-Collection(e2,e1): 612\n",
      "Entity-Origin(e1,e2): 568\n",
      "Cause-Effect(e1,e2): 344\n",
      "Component-Whole(e1,e2): 470\n",
      "Message-Topic(e2,e1): 144\n",
      "Product-Producer(e1,e2): 323\n",
      "Entity-Origin(e2,e1): 148\n",
      "Content-Container(e2,e1): 166\n",
      "Instrument-Agency(e1,e2): 97\n",
      "Entity-Destination(e2,e1): 1\n",
      "\n",
      "Test set relation distribution:\n",
      "Message-Topic(e1,e2): 210\n",
      "Product-Producer(e2,e1): 123\n",
      "Instrument-Agency(e2,e1): 134\n",
      "Entity-Destination(e1,e2): 291\n",
      "Cause-Effect(e2,e1): 194\n",
      "Component-Whole(e1,e2): 162\n",
      "Product-Producer(e1,e2): 108\n",
      "Member-Collection(e2,e1): 201\n",
      "Other: 454\n",
      "Entity-Origin(e1,e2): 211\n",
      "Content-Container(e1,e2): 153\n",
      "Entity-Origin(e2,e1): 47\n",
      "Cause-Effect(e1,e2): 134\n",
      "Component-Whole(e2,e1): 150\n",
      "Content-Container(e2,e1): 39\n",
      "Instrument-Agency(e1,e2): 22\n",
      "Message-Topic(e2,e1): 51\n",
      "Member-Collection(e1,e2): 32\n",
      "Entity-Destination(e2,e1): 1\n",
      "----------Splitted Datasets distribution-----------\n",
      "Training set relation distribution:\n",
      "Counter({'Other': 1410, 'Entity-Destination(e1,e2)': 844, 'Cause-Effect(e2,e1)': 659, 'Member-Collection(e2,e1)': 612, 'Entity-Origin(e1,e2)': 568, 'Message-Topic(e1,e2)': 490, 'Component-Whole(e2,e1)': 471, 'Component-Whole(e1,e2)': 470, 'Instrument-Agency(e2,e1)': 407, 'Product-Producer(e2,e1)': 394, 'Content-Container(e1,e2)': 374, 'Cause-Effect(e1,e2)': 344, 'Product-Producer(e1,e2)': 323, 'Content-Container(e2,e1)': 166, 'Entity-Origin(e2,e1)': 148, 'Message-Topic(e2,e1)': 144, 'Instrument-Agency(e1,e2)': 97, 'Member-Collection(e1,e2)': 78, 'Entity-Destination(e2,e1)': 1})\n",
      "\n",
      "Validation set relation distribution:\n",
      "Counter({'Other': 272, 'Entity-Destination(e1,e2)': 175, 'Entity-Origin(e1,e2)': 127, 'Message-Topic(e1,e2)': 126, 'Member-Collection(e2,e1)': 121, 'Cause-Effect(e2,e1)': 116, 'Component-Whole(e1,e2)': 97, 'Content-Container(e1,e2)': 92, 'Component-Whole(e2,e1)': 90, 'Cause-Effect(e1,e2)': 80, 'Instrument-Agency(e2,e1)': 80, 'Product-Producer(e2,e1)': 74, 'Product-Producer(e1,e2)': 65, 'Message-Topic(e2,e1)': 31, 'Entity-Origin(e2,e1)': 28, 'Content-Container(e2,e1)': 23, 'Member-Collection(e1,e2)': 19, 'Instrument-Agency(e1,e2)': 13})\n",
      "\n",
      "Test set relation distribution:\n",
      "Counter({'Other': 182, 'Entity-Destination(e1,e2)': 116, 'Entity-Origin(e1,e2)': 84, 'Message-Topic(e1,e2)': 84, 'Member-Collection(e2,e1)': 80, 'Cause-Effect(e2,e1)': 78, 'Component-Whole(e1,e2)': 65, 'Content-Container(e1,e2)': 61, 'Component-Whole(e2,e1)': 60, 'Instrument-Agency(e2,e1)': 54, 'Cause-Effect(e1,e2)': 54, 'Product-Producer(e2,e1)': 49, 'Product-Producer(e1,e2)': 43, 'Message-Topic(e2,e1)': 20, 'Entity-Origin(e2,e1)': 19, 'Content-Container(e2,e1)': 16, 'Member-Collection(e1,e2)': 13, 'Instrument-Agency(e1,e2)': 9, 'Entity-Destination(e2,e1)': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define the relation labels dictionary\n",
    "relations = [\n",
    "    \"Cause-Effect(e1,e2)\", \"Cause-Effect(e2,e1)\",\n",
    "    \"Component-Whole(e1,e2)\", \"Component-Whole(e2,e1)\",\n",
    "    \"Content-Container(e1,e2)\", \"Content-Container(e2,e1)\",\n",
    "    \"Entity-Destination(e1,e2)\", \"Entity-Destination(e2,e1)\",\n",
    "    \"Entity-Origin(e1,e2)\", \"Entity-Origin(e2,e1)\",\n",
    "    \"Instrument-Agency(e1,e2)\", \"Instrument-Agency(e2,e1)\",\n",
    "    \"Member-Collection(e1,e2)\", \"Member-Collection(e2,e1)\",\n",
    "    \"Message-Topic(e1,e2)\", \"Message-Topic(e2,e1)\",\n",
    "    \"Product-Producer(e1,e2)\", \"Product-Producer(e2,e1)\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(relations)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# 1. Load dataset from Hugging Face\n",
    "ds = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "# 2. Read training and test data\n",
    "train_texts_all = ds[\"train\"][\"sentence\"]  # Training texts\n",
    "train_labels_all = ds[\"train\"][\"relation\"]  # Training labels\n",
    "test_texts = ds[\"test\"][\"sentence\"]         # Test texts\n",
    "test_labels = ds[\"test\"][\"relation\"]        # Test labels\n",
    "\n",
    "# Get the distribution of relation labels in the training and test sets and convert to label names using id2label\n",
    "train_relation_counts = Counter([id2label[label] for label in train_labels_all])\n",
    "test_relation_counts = Counter([id2label[label] for label in test_labels])\n",
    "print(\"----------Original Datasets distribution-----------\")\n",
    "# Print relation label distribution\n",
    "print(\"Training set relation distribution:\")\n",
    "for label, count in train_relation_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "print(\"\\nTest set relation distribution:\")\n",
    "for label, count in test_relation_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "print(\"----------Splitted Datasets distribution-----------\")\n",
    "# Get the relation distribution in the training, validation, and test sets\n",
    "train_relations = [item[\"relation\"] for item in train_data]\n",
    "valid_relations = [item[\"relation\"] for item in valid_data]\n",
    "test_relations = [item[\"relation\"] for item in test_data]\n",
    "\n",
    "# Count the occurrences of each label\n",
    "train_relation_counts = Counter(train_relations)\n",
    "valid_relation_counts = Counter(valid_relations)\n",
    "test_relation_counts = Counter(test_relations)\n",
    "\n",
    "# Print the distribution results\n",
    "print(\"Training set relation distribution:\")\n",
    "print(train_relation_counts)\n",
    "\n",
    "print(\"\\nValidation set relation distribution:\")\n",
    "print(valid_relation_counts)\n",
    "\n",
    "print(\"\\nTest set relation distribution:\")\n",
    "print(test_relation_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef4a8a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation to ID mapping: {'Other': 0, 'Cause-Effect(e1,e2)': 1, 'Cause-Effect(e2,e1)': 2, 'Component-Whole(e1,e2)': 3, 'Component-Whole(e2,e1)': 4, 'Content-Container(e1,e2)': 5, 'Content-Container(e2,e1)': 6, 'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8, 'Entity-Origin(e1,e2)': 9, 'Entity-Origin(e2,e1)': 10, 'Instrument-Agency(e1,e2)': 11, 'Instrument-Agency(e2,e1)': 12, 'Member-Collection(e1,e2)': 13, 'Member-Collection(e2,e1)': 14, 'Message-Topic(e1,e2)': 15, 'Message-Topic(e2,e1)': 16, 'Product-Producer(e1,e2)': 17, 'Product-Producer(e2,e1)': 18}\n",
      "ID to Relation mapping: {0: 'Other', 1: 'Cause-Effect(e1,e2)', 2: 'Cause-Effect(e2,e1)', 3: 'Component-Whole(e1,e2)', 4: 'Component-Whole(e2,e1)', 5: 'Content-Container(e1,e2)', 6: 'Content-Container(e2,e1)', 7: 'Entity-Destination(e1,e2)', 8: 'Entity-Destination(e2,e1)', 9: 'Entity-Origin(e1,e2)', 10: 'Entity-Origin(e2,e1)', 11: 'Instrument-Agency(e1,e2)', 12: 'Instrument-Agency(e2,e1)', 13: 'Member-Collection(e1,e2)', 14: 'Member-Collection(e2,e1)', 15: 'Message-Topic(e1,e2)', 16: 'Message-Topic(e2,e1)', 17: 'Product-Producer(e1,e2)', 18: 'Product-Producer(e2,e1)'}\n",
      "Total number of relations: 19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration class: used to store the data directory path\n",
    "class Config:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir  # Set the data directory path\n",
    "\n",
    "# Relation loader class: used to read relation mappings\n",
    "class RelationLoader(object):\n",
    "    def __init__(self, config):\n",
    "        self.data_dir = config.data_dir  # Set the data directory path\n",
    "\n",
    "    def __load_relation(self):\n",
    "        \"\"\"Private method: Load the relation-to-ID mapping\"\"\"\n",
    "        relation_file = os.path.join(self.data_dir, 'relation2id.txt')  # Path to the relation file\n",
    "        rel2id = {}  # Dictionary mapping relations to IDs\n",
    "        id2rel = {}  # Dictionary mapping IDs to relations\n",
    "        with open(relation_file, 'r', encoding='utf-8') as fr:\n",
    "            for line in fr:\n",
    "                relation, id_s = line.strip().split()  # Read relation name and corresponding ID\n",
    "                id_d = int(id_s)  # Convert ID to an integer\n",
    "                rel2id[relation] = id_d  # Store in the rel2id dictionary\n",
    "                id2rel[id_d] = relation  # Store in the id2rel dictionary\n",
    "        return rel2id, id2rel, len(rel2id)  # Return mappings and total number of relations\n",
    "\n",
    "    def get_relation(self):\n",
    "        \"\"\"Retrieve relation mappings\"\"\"\n",
    "        return self.__load_relation()\n",
    "\n",
    "# Use RelationLoader to read relation2id.txt\n",
    "config = Config(data_dir=\"./\")  # Assume relation2id.txt is in the current directory\n",
    "relation_loader = RelationLoader(config)\n",
    "rel2id, id2rel, total_relations = relation_loader.get_relation()\n",
    "\n",
    "# Print results\n",
    "print(\"Relation to ID mapping:\", rel2id)\n",
    "print(\"ID to Relation mapping:\", id2rel)\n",
    "print(\"Total number of relations:\", total_relations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b24ab3-bb15-4aa8-b59e-226fc365a033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
